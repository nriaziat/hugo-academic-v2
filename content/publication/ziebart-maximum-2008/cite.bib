@inproceedings{ziebart_maximum_2008,
 abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
 author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
 booktitle = {AAAI Conference on Artificial Intelligence},
 file = {Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:/Users/naveedriaziat/Zotero/storage/RWQYFCSW/Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:application/pdf},
 language = {en},
 title = {Maximum Entropy Inverse Reinforcement Learning},
 year = {2008}
}
