---
title: 'Crowd-Sourced Assessment of Technical Skills: a novel method to evaluate surgical
  performance'
authors:
- Carolyn Chen
- Lee White
- Timothy Kowalewski
- Rajesh Aggarwal
- Chris Lintott
- Bryan Comstock
- Katie Kuksenok
- Cecilia Aragon
- Daniel Holst
- Thomas Lendvay
date: '2014-03-01'
publishDate: '2024-11-15T17:07:01.932301Z'
publication_types:
- article-journal
publication: '*The Journal of Surgical Research*'
doi: 10.1016/j.jss.2013.09.024
abstract: "BACKGROUND: Validated methods of objective assessments of surgical skills
  are resource intensive. We sought to test a web-based grading tool using crowdsourcing
  called Crowd-Sourced Assessment of Technical Skill. MATERIALS AND METHODS: Institutional
  Review Board approval was granted to test the accuracy of Amazon.com's Mechanical
  Turk and Facebook crowdworkers compared with experienced surgical faculty grading
  a recorded dry-laboratory robotic surgical suturing performance using three performance
  domains from a validated assessment tool. Assessor free-text comments describing
  their rating rationale were used to explore a relationship between the language
  used by the crowd and grading accuracy. RESULTS: Of a total possible global performance
  score of 3-15, 10 experienced surgeons graded the suturing video at a mean score
  of 12.11 (95% confidence interval [CI], 11.11-13.11). Mechanical Turk and Facebook
  graders rated the video at mean scores of 12.21 (95% CI, 11.98-12.43) and 12.06
  (95% CI, 11.57-12.55), respectively. It took 24 h to obtain responses from 501 Mechanical
  Turk subjects, whereas it took 24 d for 10 faculty surgeons to complete the 3-min
  survey. Facebook subjects (110) responded within 25 d. Language analysis indicated
  that crowdworkers who used negation words (i.e., \\\"but,\\\" \\\"although,\\\"
  and so forth) scored the performance more equivalently to experienced surgeons than
  crowdworkers who did not (P textless 0.00001). CONCLUSIONS: For a robotic suturing
  performance, we have shown that surgery-naive crowdworkers can rapidly assess skill
  equivalent to experienced faculty surgeons using Crowd-Sourced Assessment of Technical
  Skill. It remains to be seen whether crowds can discriminate different levels of
  skill and can accurately assess human surgery performances."
tags:
- Humans
- Suture Techniques
- Training
- Education
- Robotics
- Adult
- Young Adult
- Robotic surgery
- Competency-Based Education
- Crowdsourcing
- Data Collection
- Depth Perception
- Educational Measurement
- GEARS
- General Surgery
- Internet
- Internship and Residency
- OSATS
- Reference Standards
---
