@article{li_infrared_2021,
 abstract = {In this paper, we propose a new infrared and visible image fusion method based on multi-scale transformation and norm optimization. In this method, a new loss function is designed with contrast fidelity (L2 norm) and sparse constraint (L1 norm), and the split Bregman method is used to optimize the loss function to obtain pre-fusion images. The final fused base layer is obtained by using a multi-level decomposition latent low-rank representation (MDLatLRR) method to decompose the pre-fusion images. Then, using the pre-fusion image as the reference, image structure similarity (SSIM) is introduced to evaluate the validity of detail information from the visible image, and the SSIM is then transformed into a weight map which is applied to the optimization method based on L2 norm to generate the final detail fusion layer. Our proposed method is evaluated and compared with 18 state-of-the-art image fusion methods, both qualitatively and quantitatively on four public datasets (i.e., CVC14 driving dataset, TNO dataset with natural scenarios, RoadScene dataset, and whole brain atlas dataset). The results show that our proposed method is generally better than the compared methods in terms of highlighting targets and retaining effective detail information.},
 author = {Li, Guofa and Lin, Yongjie and Qu, Xingda},
 doi = {10.1016/j.inffus.2021.02.008},
 file = {ScienceDirect Snapshot:/Users/naveedriaziat/Zotero/storage/S6TJSWAB/S1566253521000221.html:text/html},
 issn = {1566-2535},
 journal = {Information Fusion},
 keywords = {Image fusion, Infrared image, Visible image, Intelligent systems, Pre-fusion image},
 month = {July},
 pages = {109--129},
 title = {An infrared and visible image fusion method based on multi-scale transformation and norm optimization},
 url = {https://www.sciencedirect.com/science/article/pii/S1566253521000221},
 urldate = {2024-03-13},
 volume = {71},
 year = {2021}
}
