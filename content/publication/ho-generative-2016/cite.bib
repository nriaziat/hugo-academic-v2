@misc{ho_generative_2016,
 abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
 author = {Ho, Jonathan and Ermon, Stefano},
 doi = {10.48550/arXiv.1606.03476},
 file = {arXiv Fulltext PDF:/Users/naveedriaziat/Zotero/storage/GEHTYF8V/Ho and Ermon - 2016 - Generative Adversarial Imitation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/naveedriaziat/Zotero/storage/U43WLI3X/1606.html:text/html},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
 month = {June},
 note = {arXiv:1606.03476 [cs]},
 publisher = {arXiv},
 title = {Generative Adversarial Imitation Learning},
 url = {http://arxiv.org/abs/1606.03476},
 urldate = {2023-11-13},
 year = {2016}
}
