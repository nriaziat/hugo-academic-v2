---
title: Generative Adversarial Imitation Learning
authors:
- Jonathan Ho
- Stefano Ermon
date: '2016-06-01'
publishDate: '2024-11-15T17:07:01.666146Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.1606.03476
abstract: Consider learning a policy from example expert behavior, without interaction
  with the expert or access to reinforcement signal. One approach is to recover the
  expert's cost function with inverse reinforcement learning, then extract a policy
  from that cost function with reinforcement learning. This approach is indirect and
  can be slow. We propose a new general framework for directly extracting a policy
  from data, as if it were obtained by reinforcement learning following inverse reinforcement
  learning. We show that a certain instantiation of our framework draws an analogy
  between imitation learning and generative adversarial networks, from which we derive
  a model-free imitation learning algorithm that obtains significant performance gains
  over existing model-free methods in imitating complex behaviors in large, high-dimensional
  environments.
tags:
- Computer Science - Machine Learning
- Computer Science - Artificial Intelligence
links:
- name: URL
  url: http://arxiv.org/abs/1606.03476
---
